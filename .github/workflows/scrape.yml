```yaml
     ---
     name: Auto-Scrape Awesome Lists
     on:
       schedule:
         - cron: '0 0 * * *'  # Runs daily at midnight UTC
       workflow_dispatch:  # Allows manual trigger
     jobs:
       scrape:
         runs-on: ubuntu-latest
         steps:
           - name: Checkout repo
             uses: actions/checkout@v4
             with:
               fetch-depth: 0
               ref: master
           - name: Set up Python
             uses: actions/setup-python@v5
             with:
               python-version: '3.9'
           - name: Install dependencies
             run: |
               python -m pip install --upgrade pip
               pip install scrapy frontmatter lxml
           - name: Debug file structure before scrape
             run: |
               ls -R
               echo "Checking awesome.txt:"
               cat extension/data/awesome.txt || echo "File not found or empty"
           - name: Run scraper to update data
             run: python scraper/awesome_scraper.py
             env:
               GITHUB_TOKEN: ${{ github.token }}
           - name: Debug file structure after scrape
             run: |
               ls -R
               echo "Checking awesome.txt after scrape:"
               cat extension/data/awesome.txt || echo "File not found or empty"
           - name: Commit and push updated data
             run: |
               git config user.name "Automated Scraper"
               git config user.email "actions@github.com"
               git add extension/data/awesome.txt || echo "No awesome.txt to add"
               git commit -m "Auto-update scraped content: $(date -u)" || echo "No changes to commit"
               git push
     ```
